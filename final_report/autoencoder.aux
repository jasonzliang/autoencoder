\relax 
\citation{bengio2012rep}
\citation{vincent2010stacked}
\citation{bengio2012rep}
\citation{vincent2010stacked}
\citation{hinton2006learning}
\citation{hecht1989theory,bottou-91c}
\citation{srinivas1994genetic}
\citation{cantu1998survey}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Overview of an autoencoder and its encoding, decoding stages. The weight matrix of the decoding stage is the transpose of the weight matrix of the encoding stage.}}{1}}
\newlabel{fig:autoencoder}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{1}}
\citation{hecht1989theory}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Performance results on a single autoencoder layer with 500 hidden nodes and trained for 15 iterations. Plot shows time elapsed versus total training error over 5000 images for 1, 4, and 8 threads.}}{2}}
\newlabel{fig:experiment1}{{2}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Algorithm Description}{2}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Table giving notation for the derivation of updates.}}{2}}
\newlabel{tab:notation}{{I}{2}}
\citation{vincent2010stacked}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Backpropogation}}{3}}
\newlabel{alg:backprop}{{1}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Experimental Results}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Performance of Stochastic Gradient Descent Training}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Time per iteration versus number of threads and hidden nodes. All 60000 training images are used.}}{3}}
\newlabel{fig:experiment2}{{3}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Visualization of the filters of the first 100 hidden nodes in an denoising autoencoder trained over all 60000 images.}}{3}}
\newlabel{fig:experiment3_1}{{4}{3}}
\citation{vincent2010stacked}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Visualization of Autoencoder}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}}Representation Learning for Supervised Classification}{4}}
\newlabel{tab:classvsenc}{{\unhbox \voidb@x \hbox {IV-C}}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Summary of the results of running different classifcation algorithms on the raw MNIST data and on the output from a trained autoencoder. We see in all cases that using the encoded data produces a better result.}}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-D}}Training the Autencoder with a Genetic Algorithm}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Comparison of the performance of SGD, HGA, and CGA. SGD is fastest, while HGA achieves the lowest reconstruction error.}}{4}}
\newlabel{fig:ga_comparison}{{5}{4}}
\citation{srinivas1994genetic}
\citation{cantu1998survey}
\citation{vincent2010stacked}
\bibstyle{unsrt}
\bibdata{autoencoder}
\bibcite{bengio2012rep}{1}
\bibcite{vincent2010stacked}{2}
\bibcite{hinton2006learning}{3}
\bibcite{hecht1989theory}{4}
\bibcite{bottou-91c}{5}
\bibcite{srinivas1994genetic}{6}
\bibcite{cantu1998survey}{7}
\@writefile{toc}{\contentsline {section}{\numberline {V}Future Work}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Visualization of the reconstruction capabilities of an denoising autoencoder with 500 hidden units. Odd columns show noisy digit input images, even columns show reconstructed outputs.}}{5}}
\newlabel{fig:experiment3_2}{{6}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{5}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Genetic Algorithm}}{5}}
\newlabel{alg:genetic}{{2}{5}}
\@writefile{toc}{\contentsline {section}{References}{5}}
