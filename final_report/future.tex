As mentioned above, genetic algorithms are sensitive to their parameters. Consequently, much time and effort may be required in order to determine the optimal parameters and achieve good convergence performance. Without the correct pararmeters, the genetic algorithm may not converge to a solution at all or may converge very slowly. A technique known as metaoptimization could be used to automatically determine the correct parameters for performing the optimization. Grefenstette discusses this metaoptimization over six possible parameters for the genetic algorithm \cite{metaoptimization}. Finding the best possible parameters may result in better convergence for our genetic algorithms in both rate of error decrease and minimum error achieved.

Our experiments and results are based on shared memory, single machine parallelism. An interesting future topic to explore is expanding the methods we considered to more cores and more machines. Expanding to more cores is a trivial extension requiring just the appropriate multicore hardware with more than 16 processor cores on a node. Expanding what we have done to a distributed setting would require more work, and rewriting much of the code. 
