Dimensionality reduction of high dimensional data is a common problem in machine learning with many applications. Given input points embedded in high dimensional space, the goal of dimensionality reduction is to learn a lower dimensional manifold that fits most of the points well. Simpler methods such as principal component analysis (PCA) \cite{wold1987principal} work by finding the orthogonal directions that best explains the variance of the input data. These directions or ``principal components'' can be then used to transform the input data by mapping it to coordinates along the principal components. The main drawback of PCA is that it only learn a linear transformation, which means PCA performs poorly when the input data do not lie on a linear manifold (a hyperplane). More sophisticated variants such as kernel PCA \cite{scholkopf1997kernel} deal with nonlinear input by using a nonlinear transformation to map the input data points into a higher dimensional space where they are approximately linear. Other nonlinear manifold learning algorithms include Isomap \cite{tenenbaum2000global} and t-SNE \cite{van2008visualizing}. These algorithms use heuristics to compute a similarity metric between the input points and then perform multidimensional scaling. Unfortunately, all of these methods mentioned above only work well if the manifold is locally smooth and often degrade in performance for highly varying manifolds \cite{van2008visualizing}.

Autoencoders, also known as auto-associators, are a class of feed-forward neural networks that are designed for performing dimensionality reduction and manifold learning \cite{hinton2006reducing}. Artificial neural networks are biologically inspired computational structures that are capable of approximating functions of arbitrary shape and dimensionality \cite{haykin2004comprehensive}. Unlike normal neural networks for regression or classification, autoencoders are trained in an unsupervised manner. The weights of an autoencoder can be trained with any optimization algorithm, but the mostly commonly used one is gradient descent, also known as the backpropagation algorithm \cite{hecht1989theory,bottou-91c}. 

Most recent research on autoencoders focus on training them to perform representation learning \cite{bengio2012rep}. Representation learning is a generalization of manifold learning where the goal is not only to discover a lower dimensional substructure, but also to learn a higher dimensional but sparser and more linearly separable representation of the input data. It has been shown that training autoencoders to denoise is one way to learn good representations of the data \cite{vincent2010stacked}. Similarly, adding a regularization term that penalizes the Frobenius norm of the Jacobian matrix of the encoder activations is also effective \cite{rifai2011contractive}. Furthermore, by stacking autoencoders, it has been shown that the resulting deep neural network is able to overcome the difficulties associated with existing dimensionality reduction methods in learning highly varying manifolds \cite{van2008visualizing,vincent2010stacked}.

A related research area to representation learning is deep learning, where deep neural networks are trained such that each subsequent layer learns a higher level representation of the input of the previous data \cite{bengio2009learning}. It has been shown with each additional layer, the transformed input feature steadily becomes more global and less invariant to local distortions. Compared to shallow neural networks, deep neural networks perform and generalize better on classification tasks, especially for high dimensional inputs. In fact for many standard datasets, their performance is state of the art by a wide margin \cite{schmidhuber2014deep}. 

Besides stacked autoencoders, the other two major deep learning architectures are stacked restricted Boltzmann machines (RBMs) \cite{hinton2006fast} and convolutional neural networks (CNNs) \cite{lecun1995convolutional}. A RBM is a undirected graphical model that is trained to learn the distribution of the input data. The special bipartite structure of the RBM makes Gibbs sampling tractable and gives rise to a fast training algorithm called contrastive divergence \cite{hinton2006fast,hinton2006learning}. Like autoencoders, RBMs can also be stacked in a similar manner and converted into a deterministic deep neural network \cite{hinton2006reducing}. CNNs are a type of deep neural network that is especially designed for processing image data and are composed of two main types of layers: convolutional and max pooling. In the convolutional layer, local filters of weights are convolved with the input while in the max pooling layer, the input is down-sampled. CNNs are trained in a supervised manner with backpropagation. Due to the sparsity of the architecture, CNNs can be efficiently trained for input data with millions of dimensions and consequently, achieve state of the art performance in many large scale image classification benchmarks \cite{ciresan2011committee,krizhevsky2012imagenet,goodfellow2013multi}.

A survey of GAs and how they perform optimization can be found in \cite{srinivas1994genetic}. Much research has be done in applying GAs for optimization of weights in neural networks \cite{gomez2006efficient,floreano2008neuroevolution}. There is a small amount of recent work on applying GAs and evolutionary algorithms in general to deep learning \cite{koutnik2014evolving,david2014genetic}. Cant{\'u}-Paz discusses methods for parallelizing GAs, which will be essential to scalability when using a GA to train our autoencoder \cite{cantu1998survey}.

