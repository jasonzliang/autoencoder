\documentclass[conference,onecolumn]{IEEEtran}

\usepackage{cite}
\usepackage[pdftex]{graphicx}
\graphicspath{{figures/}}
\usepackage[cmex10]{amsmath}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage{fixltx2e}
\usepackage{stfloats}
\usepackage{url}
\usepackage{amssymb}
\usepackage{float}
\usepackage{color}
\usepackage{balance}
\usepackage{subfig}
\usepackage{multirow}
\usepackage[margin=1in]{geometry}
\usepackage{placeins} %enables float barrier
%\usepackage{caption} %uncomment these for more space between a table and its caption
%\captionsetup[table]{skip=5pt}

\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

\begin{document}

\title{Trained Stacked Denoising Autoencoders for Representation Learning}

\author{\IEEEauthorblockN{Jason Liang}
\IEEEauthorblockA{jasonzliang@utexas.edu}
\and
\IEEEauthorblockN{Keith Kelly}
\IEEEauthorblockA{keith@ices.utexas.edu}}

\maketitle

\begin{abstract}
We implement stacked denoising autoencoders, a class of neural networks that are capable of learning powerful representations of high dimensional data. We describe stochastic gradient descent for unsupervised training of autoencoders, as well as a novel genetic algorithm based approach that makes use of gradient information. We analyze the performance of both optimization algorithms and also the representation learning ability of the autoencoder when it is trained on standard image classification datasets. 
\end{abstract}

\FloatBarrier
\section{Introduction}
\input{intro}
\FloatBarrier
\section{Related Work}
\input{related}
\FloatBarrier
\section{Algorithm Description}
\input{algo}
\FloatBarrier

\section{Experimental Results}
\input{experiment}
\FloatBarrier
\section{Discussion}
% \input{reconstructed}
\input{discussion}
\FloatBarrier
\section{Future Work}
\input{future}
\FloatBarrier
\section{Conclusion}
\input{conclusion}
\FloatBarrier

\bibliographystyle{unsrt}
\bibliography{autoencoder}

\end{document}


