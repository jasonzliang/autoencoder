\relax 
\citation{bengio2012rep}
\citation{vincent2010stacked}
\citation{bengio2012rep}
\citation{vincent2010stacked}
\citation{hinton2006learning}
\citation{hecht1989theory,bottou-91c}
\citation{srinivas1994genetic}
\citation{cantu1998survey}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Overview of an autoencoder and its encoding, decoding stages. The weight matrix of the decoding stage is the transpose of the weight matrix of the encoding stage.\relax }}{1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:autoencoder}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{1}}
\citation{hecht1989theory}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Performance results on a single autoencoder layer with 500 hidden nodes and trained for 15 iterations. Plot shows time elapsed versus total training error over 5000 images for 1, 4, and 8 threads.\relax }}{2}}
\newlabel{fig:experiment1}{{2}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Algorithm Description}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Stochastic Gradient Descent}{2}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Table giving notation for the derivation of updates.\relax }}{2}}
\newlabel{tab:notation}{{I}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces reconstructions of corrupted digits from the (a) MNIST dataset (b) bg-rand dataset, (c) bg-img dataset, and (d) rot dataset\relax }}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {a}}}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {a}}}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {a}}}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {a}}}{3}}
\newlabel{fig:reconstruct}{{3}{3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Backpropogation\relax }}{4}}
\newlabel{alg:backprop}{{1}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Performance}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Time per iteration versus the number of threads and hidden nodes. We use our own parallel implementation using OpenMP and we use 5000 training images.\relax }}{4}}
\newlabel{fig:performanceomp}{{4}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Genetic Algorithm}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Time per iteration versus the number of threads and hidden nodes. We parallelize with OpenBLAS and we use 5000 training images.\relax }}{5}}
\newlabel{fig:performanceblas}{{5}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Time per iteration versus parallelization technique and hidden nodes. We use 16 threads here.\relax }}{5}}
\newlabel{fig:ompvsblas}{{6}{5}}
\citation{vincent2010stacked}
\citation{vincent2010stacked}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Experimental Results}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Performance of Stochastic Gradient Descent Training}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Visualization of the filters of the first 100 hidden nodes in an denoising autoencoder trained over all 60000 images.\relax }}{6}}
\newlabel{fig:experiment3_1}{{7}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Visualization of Autoencoder}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}}Representation Learning for Supervised Classification}{6}}
\newlabel{tab:classvsenc}{{\caption@xref {tab:classvsenc}{ on input line 117}}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Summary of the results of running different classification algorithms on the raw MNIST data and on the output from a trained autoencoder. We see in all cases that using the encoded data produces a better result.\relax }}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Comparison of the performance of SGD, HGA, and CGA. SGD is fastest, while HGA achieves the lowest reconstruction error.\relax }}{7}}
\newlabel{fig:ga_comparison}{{8}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-D}}Training the Autoencoder with a Genetic Algorithm}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Performance of HGA for 1, 4, 8, and 16 threads.\relax }}{7}}
\newlabel{fig:ga_comparison2}{{9}{7}}
\citation{srinivas1994genetic}
\citation{cantu1998survey}
\citation{vincent2010stacked}
\bibstyle{unsrt}
\bibdata{autoencoder}
\bibcite{bengio2012rep}{1}
\bibcite{vincent2010stacked}{2}
\bibcite{hinton2006learning}{3}
\bibcite{hecht1989theory}{4}
\bibcite{bottou-91c}{5}
\bibcite{srinivas1994genetic}{6}
\bibcite{cantu1998survey}{7}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Comparison of performance versus number of threads and number of hidden units in autoencoder layer for HGA with population size (a) 2 and (b) 50.\relax }}{8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{8}}
\newlabel{fig:ga_comparison3}{{10}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Future Work}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{8}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Genetic Algorithm\relax }}{8}}
\newlabel{alg:genetic}{{3}{8}}
\@writefile{toc}{\contentsline {section}{References}{8}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Genetic Algorithm\relax }}{9}}
\newlabel{alg:genetic}{{2}{9}}
