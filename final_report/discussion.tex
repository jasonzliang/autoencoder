The experimental results demonstrate the importance of learning a better representation of the data for the classification algorithms given. While the kernel SVM performed comparably with and without the improved representation of the data, it is also much more expensive than the linear SVM or logistic regression methods. Thus the tradeoff becomes whether to use a more expensive classifier, or to learn a better representation and then use a cheaper classifier. It is also important to consider that the autoencoder combined with a simple classifier performed better than the kernel SVM in several cases.

As we saw in the results section, SGD is the fastest method for training the autoencoder, in terms of walltime. That is, to achieve a given error rate, using SGD will be faster than HGA or CGA in terms of time (as opposed to number of iterations). This might suggest that SGD is the preferred method for training an autoencoder, but it does not come without drawbacks. In terms of the number of iterations to achieve a given error rate, we saw that HGA performed the best, with SGD second and CGA last. This is important because of how the methods scale. SGD scales decently with respect to increasing number of threads, but the genetic algorithms demonstrate better scaling properties, being closer to linear in their scaling. 
Furthermore, CGA is able to achieve the lowest error rate of any of the methods considered. Since the reconstruction error is an indication of of how well the autoencoder encodes (and decodes) the input, the measures the quality of the learned representation of the data.

However, genetic algorithms have many parameters that need to be tuned and are problem dependent. In contrast, SGD has only one: the learning rate. Consequently it can sometimes be difficult to get GAs to converge at all or at a reasonable rate without knowing the correct parameters. Thus it may be interesting to consider a metaoptimization of the parameters for a genetic algorithm, so that this tuning may be done automatically. 

