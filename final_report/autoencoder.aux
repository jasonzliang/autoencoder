\relax 
\citation{bengio2012rep}
\citation{vincent2010stacked}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\citation{bengio2012rep}
\citation{vincent2010stacked}
\citation{hinton2006learning}
\citation{hecht1989theory,bottou-91c}
\citation{srinivas1994genetic}
\citation{cantu1998survey}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Overview of an autoencoder and its encoding, decoding stages. The weight matrix of the decoding stage is the transpose of the weight matrix of the encoding stage.\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:autoencoder}{{1}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{2}}
\citation{hecht1989theory}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces reconstructions of corrupted digits from the (a) MNIST dataset (b) bg-rand dataset, (c) bg-img dataset, and (d) rot dataset\relax }}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{3}}
\newlabel{fig:reconstruct}{{2}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Algorithm Description}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Stochastic Gradient Descent}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Table giving notation for the derivation of updates.\relax }}{4}}
\newlabel{tab:notation}{{I}{4}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Backpropagation\relax }}{5}}
\newlabel{alg:backprop}{{1}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Performance}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Time per iteration versus the number of threads and hidden nodes. We use our own parallel implementation using OpenMP and we use 5000 training images.\relax }}{6}}
\newlabel{fig:performanceomp}{{3}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Genetic Algorithm}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Time per iteration versus the number of threads and hidden nodes. We parallelize with OpenBLAS and we use 5000 training images.\relax }}{7}}
\newlabel{fig:performanceblas}{{4}{7}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Conventional Genetic Algorithm (CGA)\relax }}{7}}
\newlabel{alg:genetic}{{2}{7}}
\citation{david2014genetic}
\citation{david2014genetic}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Time per iteration versus parallelization technique and hidden nodes. We use 16 threads here.\relax }}{8}}
\newlabel{fig:ompvsblas}{{5}{8}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Hybrid Genetic Algorithm (HGA)\relax }}{8}}
\newlabel{alg:hga}{{3}{8}}
\citation{vincent2010stacked}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Experimental Results}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Performance of Stochastic Gradient Descent Training}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Visualization of the filters of the first 100 hidden nodes in an denoising autoencoder trained over all 60000 images.\relax }}{9}}
\newlabel{fig:experiment3_1}{{6}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Performance results on a single autoencoder layer with 500 hidden nodes and trained for 15 iterations. Plot shows time elapsed versus total training error over 5000 images for 1, 4, and 8 threads.\relax }}{10}}
\newlabel{fig:experiment1}{{7}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Visualization of Autoencoder}{10}}
\citation{vincent2010stacked}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}}Representation Learning for Supervised Classification}{11}}
\newlabel{tab:classvsenc}{{\caption@xref {tab:classvsenc}{ on input line 153}}{11}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Summary of the results of running different classification algorithms on the raw MNIST data and on the output from a trained autoencoder. We see in all cases that using the encoded data produces a better result.\relax }}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-D}}Training the Autoencoder with a Genetic Algorithm}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Comparison of the performance of SGD, HGA, and CGA. SGD is fastest, while HGA achieves the lowest reconstruction error.\relax }}{12}}
\newlabel{fig:ga_comparison}{{8}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Performance of HGA for 1, 4, 8, and 16 threads.\relax }}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Comparison of the performance of SGD, HGA, and CGA. SGD is fastest, while HGA achieves the lowest reconstruction error.\relax }}{13}}
\newlabel{fig:ga_comparison2}{{10}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Comparison of performance versus number of threads and number of hidden units in autoencoder layer for HGA with population size (a) 2 and (b) 50.\relax }}{14}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{14}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{14}}
\newlabel{fig:ga_comparison3}{{11}{14}}
\citation{srinivas1994genetic}
\citation{cantu1998survey}
\citation{vincent2010stacked}
\bibstyle{unsrt}
\bibdata{autoencoder}
\bibcite{bengio2012rep}{1}
\bibcite{vincent2010stacked}{2}
\bibcite{hinton2006learning}{3}
\bibcite{hecht1989theory}{4}
\bibcite{bottou-91c}{5}
\bibcite{srinivas1994genetic}{6}
\bibcite{cantu1998survey}{7}
\bibcite{david2014genetic}{8}
\@writefile{toc}{\contentsline {section}{\numberline {V}Future Work}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{15}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Genetic Algorithm\relax }}{15}}
\newlabel{alg:genetic}{{4}{15}}
\@writefile{toc}{\contentsline {section}{References}{15}}
